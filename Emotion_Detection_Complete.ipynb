{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30d7879a",
   "metadata": {},
   "source": [
    "# Emotion Detection using OpenCV & TensorFlow\n",
    "Real-time facial emotion detection using MobileNetV2 and OpenCV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d98041",
   "metadata": {},
   "source": [
    "## Install Required Libraries\n",
    "```python\n",
    "!pip install tensorflow opencv-python matplotlib\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb188022",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500e38c7",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03c0fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_gen = ImageDataGenerator(rescale=1./255, horizontal_flip=True, zoom_range=0.2)\n",
    "val_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_data = train_gen.flow_from_directory('data/train', target_size=(224,224), batch_size=32, class_mode='categorical')\n",
    "val_data = val_gen.flow_from_directory('data/validation', target_size=(224,224), batch_size=32, class_mode='categorical')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfabd268",
   "metadata": {},
   "source": [
    "## Build the Model using MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b50d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=(224,224,3), include_top=False, weights='imagenet')\n",
    "base_model.trainable = False\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(train_data.num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d85abfa",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f01f589",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=3, restore_best_weights=True),\n",
    "    ModelCheckpoint('best_emotion_model.h5', save_best_only=True)\n",
    "]\n",
    "\n",
    "history = model.fit(train_data, validation_data=val_data, epochs=20, callbacks=callbacks)\n",
    "\n",
    "# Save the trained model\n",
    "model.save('emotion_model_mobilenet')\n",
    "\n",
    "# Plot history\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.xlabel('Epoch'); plt.ylabel('Value')\n",
    "plt.legend(); plt.grid(True); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664a8f00",
   "metadata": {},
   "source": [
    "## Real-Time Emotion Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a342b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = tf.keras.models.load_model('emotion_model_mobilenet')\n",
    "class_names = list(train_data.class_indices.keys())\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "        for (x,y,w,h) in faces:\n",
    "            roi_color = frame[y:y+h, x:x+w]\n",
    "            roi_resized = cv2.resize(roi_color, (224,224))/255.0\n",
    "            roi_expanded = np.expand_dims(roi_resized, axis=0)\n",
    "            pred = model.predict(roi_expanded)\n",
    "            emotion = class_names[np.argmax(pred)]\n",
    "            confidence = np.max(pred)\n",
    "            label = f\"{emotion} ({confidence*100:.1f}%)\"\n",
    "\n",
    "            cv2.rectangle(frame, (x,y), (x+w,y+h), (255,0,0), 2)\n",
    "            cv2.putText(frame, label, (x,y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,255,0), 2)\n",
    "\n",
    "        cv2.imshow('Emotion Detection', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'): break\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "finally:\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86b0d06",
   "metadata": {},
   "source": [
    "## Predict Emotion from Static Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9a5031",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "img = image.load_img('test.jpg', target_size=(224,224))\n",
    "img_array = image.img_to_array(img)/255.0\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "pred = model.predict(img_array)\n",
    "print(\"Predicted emotion:\", class_names[np.argmax(pred)])\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
